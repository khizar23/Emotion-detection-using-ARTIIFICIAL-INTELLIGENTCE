{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khizar23/Emotion-detection-using-ARTIIFICIAL-INTELLIGENTCE/blob/main/chatbot_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1fe90751",
      "metadata": {
        "id": "1fe90751"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "import random\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from datetime import datetime, date\n",
        "import tensorflow as tf\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "277d6f9a",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "277d6f9a",
        "outputId": "cd19aef6-bf5f-439e-eda7-16d5bedbcda0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-fddd2ea23dbb>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# returns JSON object as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# a dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "f = open('/content/intent.json',)\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac0d530d",
      "metadata": {
        "id": "ac0d530d"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a5d8fc4",
      "metadata": {
        "id": "1a5d8fc4"
      },
      "outputs": [],
      "source": [
        "stop_words = ['is','am','the','a','an','be','are','were',]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a310647",
      "metadata": {
        "id": "4a310647"
      },
      "outputs": [],
      "source": [
        "\n",
        "words = []\n",
        "y = []\n",
        "patterns =[]\n",
        "x = []\n",
        "classes = []\n",
        "# getting the classes(tags) output and patterns from 'data'\n",
        "for intent in data['intents']:\n",
        "    classes.append(intent['tag'])\n",
        "    for pattern in intent['patterns']:\n",
        "# applying tokenizer to convert the sentences into a list of words\n",
        "        tokens = nltk.word_tokenize(pattern)\n",
        "# here .extend() is used instead of append() since we don't want to append lists in words\n",
        "# but its elements i.e words in 'token'\n",
        "# here 'tokens' is a list of words\n",
        "        words.extend(tokens)\n",
        "        patterns.append(pattern)\n",
        "        y.append(intent['tag'])\n",
        "\n",
        "# converting to lower case, applyging lemmatization removing the puctuations\n",
        "# here 'words' is our vocabulary containing all the words\n",
        "words = [stemmer.stem(word.lower()) for word  in words if word not in string.punctuation and\n",
        "        word not in stop_words]\n",
        "# converting the list to set to avoid doubling of words in in 'words'\n",
        "words = sorted(set(words))\n",
        "words = list(words)\n",
        "\n",
        "for list_ in patterns:\n",
        "    list_ = nltk.word_tokenize(list_)\n",
        "    list_ = [stemmer.stem(lis.lower()) for lis in list_ if lis not in string.punctuation and lis not in stop_words]\n",
        "    x.append(list_)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d469952",
      "metadata": {
        "id": "9d469952"
      },
      "outputs": [],
      "source": [
        "# applying the one hot encoding on our training data\n",
        "train_x = []\n",
        "train_y = []\n",
        "training = []\n",
        "empty_list = [0]*len(classes)\n",
        "for idx,list_ in enumerate(x):\n",
        "    doc = []\n",
        "    for word in words:\n",
        "        doc.append(1) if word in list_ else doc.append(0)\n",
        "    output = list(empty_list)\n",
        "    output[classes.index(y[idx])] = 1\n",
        "    training.append([doc,output])\n",
        "random.shuffle(training)\n",
        "training = np.array(training,dtype = object)\n",
        "train_x = list(training[:,0])\n",
        "train_x = np.array(train_x)\n",
        "train_y = np.array(list(training[:,1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "815983cd",
      "metadata": {
        "id": "815983cd"
      },
      "outputs": [],
      "source": [
        "ACCURACY_THRESHOLD = 0.99\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('accuracy') > ACCURACY_THRESHOLD):\n",
        "            print(\"\\nTraining stopped at epoch number:\",epoch)\n",
        "            self.model.stop_training = True\n",
        "callbacks = myCallback()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "473f70a7",
      "metadata": {
        "id": "473f70a7",
        "outputId": "09551dd6-4a8e-4dc8-9b32-75351689b206"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-12 23:07:26.474480: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-12-12 23:07:26.482450: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-12-12 23:07:26.482543: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (zain-Latitude-E6440): /proc/driver/nvidia/version does not exist\n",
            "2022-12-12 23:07:26.527039: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 2s 5ms/step - loss: 2.8332 - accuracy: 0.0784\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 2.6795 - accuracy: 0.1765\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 2.4373 - accuracy: 0.3725\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 2.2695 - accuracy: 0.3922\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.0134 - accuracy: 0.5490\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 1.6905 - accuracy: 0.5686\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 1.4862 - accuracy: 0.6667\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 1.2008 - accuracy: 0.7843\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 1.0062 - accuracy: 0.7843\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.7539 - accuracy: 0.8824\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.5433 - accuracy: 0.9216\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4347 - accuracy: 0.9412\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.3829 - accuracy: 0.9020\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.9608\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.2968 - accuracy: 0.9412\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.1449 - accuracy: 0.9804\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.1598 - accuracy: 0.9608\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.1396 - accuracy: 0.9804\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.1675 - accuracy: 0.9608\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.1139 - accuracy: 0.9608\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.0807 - accuracy: 0.9804\n",
            "Epoch 22/200\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0850 - accuracy: 1.0000\n",
            "Training stopped at epoch number: 21\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.0838 - accuracy: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5c99c993c0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_shape = (len(train_x[1]),)\n",
        "output_shape = len(train_y[0])\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=input_shape, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(output_shape, activation = \"softmax\"))\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(x=train_x, y=train_y, epochs=200, verbose=1,callbacks=[callbacks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8ac102",
      "metadata": {
        "id": "7b8ac102",
        "outputId": "decd1eb7-13c5-4a93-82b8-84b192ff7039"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "load_model() missing 1 required positional argument: 'filepath'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/jupyter/environment/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/jupyter/environment/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "\u001b[0;31mTypeError\u001b[0m: load_model() missing 1 required positional argument: 'filepath'"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "model = keras.models.load_model()\n",
        "model.save('model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fd1c4f6",
      "metadata": {
        "id": "0fd1c4f6"
      },
      "outputs": [],
      "source": [
        "# applying bag of words techniques to convert user's text in binary\n",
        "def bagofwords(msg):\n",
        "    tokens = nltk.word_tokenize(msg)\n",
        "    tokens = [stemmer.stem(token.lower()) for token in tokens if token not in string.punctuation and token\n",
        "              not in stop_words]\n",
        "    binary_msg = []\n",
        "    for word in words:\n",
        "        binary_msg.append(1) if word in tokens else binary_msg.append(0)\n",
        "    return np.array(binary_msg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a869fb5",
      "metadata": {
        "id": "0a869fb5"
      },
      "outputs": [],
      "source": [
        "def prediction(msg):\n",
        "\n",
        "    message = bagofwords(msg)\n",
        "    result = model.predict(np.array([message]))\n",
        "    result = np.argmax(result,axis=1)\n",
        "    class_index = list(result)[0]\n",
        "    current_tag = classes[class_index]\n",
        "\n",
        "\n",
        "    for intent in data['intents']:\n",
        "        if intent['tag'] == current_tag:\n",
        "            response = random.choice(intent['responses'])\n",
        "            return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99d2b1ca",
      "metadata": {
        "scrolled": true,
        "id": "99d2b1ca",
        "outputId": "0d42319a-0df4-40c9-ca0a-5c07c0dde6b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANN\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "ANN.txt\n",
            "Machine learning\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "machine_learning.txt\n",
            "Tell me about Augmented reality\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Augmented Reality.txt\n",
            "augmented reality\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Augmented Reality.txt\n"
          ]
        }
      ],
      "source": [
        "text = input()\n",
        "response = prediction(text)\n",
        "while True:\n",
        "    print(response)\n",
        "    text = input()\n",
        "    response = prediction(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efe457aa",
      "metadata": {
        "id": "efe457aa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}